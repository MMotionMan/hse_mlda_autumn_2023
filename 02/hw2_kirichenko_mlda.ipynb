{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6984227,"sourceType":"datasetVersion","datasetId":4013962}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install razdel\n!pip install nltk\n!pip install pymorphy2","metadata":{"id":"80Y4CVRJ3we1","outputId":"53ac38b3-bb87-4810-9e54-dc1f2417ff8f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\nnltk.download('stopwords')","metadata":{"id":"sKTJK_5R31m1","outputId":"d186de03-855b-411f-a780-6a23e4a90594","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"PqHrpGT3lDbP","outputId":"c62d6319-854c-499d-f767-e5f2357fc2cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\npath = '/kaggle/input/otbrniy/train.csv/train.csv'\n\ntrain_data = pd.read_csv(path)","metadata":{"id":"AFDTBXgXlvdB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Часть 1. Предобработка текста и решение при помощи бустинга**","metadata":{"id":"m31Und7Qn-lf"}},{"cell_type":"code","source":"train_data.head()","metadata":{"id":"eZGMR2hbp0XO","outputId":"4127b4a6-13f3-4ba6-a04b-441ae1ca8560"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Удалим из текста все символы, которые не являются буквами русского или английсского алфавитов. А также заменим последовательности пробелов на один пробел.","metadata":{"id":"hc1BsMuik6tD"}},{"cell_type":"code","source":"r = re.compile('[^A-Za-zА-Яа-я ]')\n\ncleaned_text = [re.sub(r, '', text.lower()) for text in train_data['title']]\n\nr = re.compile(' +')\ncleaned_text = [re.sub(r, ' ', text) for text in cleaned_text]\n\ntrain_data['title'] = cleaned_text","metadata":{"id":"oV3cp1FclCxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"id":"rTQdssNiT-Fu","outputId":"bdab2f98-6765-4aa7-fae9-d268aaa9c21e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data[train_data['title'] == ' '])","metadata":{"id":"SngNuotnN6cX","outputId":"a3ee6d43-56cc-4bca-fb45-b4adca089de0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"232 строки состоящих только из пробелов","metadata":{"id":"DJyynWFCV00q"}},{"cell_type":"code","source":"len(train_data[train_data['title'] == ''])","metadata":{"id":"onW0JQJIl3qV","outputId":"ecc5843d-3710-4e47-9830-a681407eda00"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"552 пустых строки","metadata":{"id":"trBqWWmiV4yb"}},{"cell_type":"code","source":"y = []\nfor i in range(5, 35, 5):\n    y.append(len(train_data.loc[train_data['title'].str.len() < i]))\n\nx = [i for i in range(len(y))]\n\nplt.bar(x, y)\nplt.xticks(y)\nplt.plot()","metadata":{"id":"x9D9VsQZOFPO","outputId":"0a422a2a-494f-4484-ede4-d84dc158bb3e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Удалим все строки, у которых длина title < 10 символов, включая пустые строки и строки состоящие только из пробелов.","metadata":{"id":"rJGxenLrWhd7"}},{"cell_type":"code","source":"train_data = train_data.loc[train_data['title'].str.len() > 10]","metadata":{"id":"R7bH85JhXbDv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"28188 текстов содержат в себе текст не на русском языке. (возможно стоит их перевести на русский, но тяжело будет выделить все языки, которые есть в датасете, как минимум еще видел записи на украинском)","metadata":{"id":"vU7hsq3GW1h8"}},{"cell_type":"code","source":"import re\n\nr = re.compile('[^А-Яа-я]')\n\nenglish = train_data[train_data['title'].isin(filter(r.match, train_data['title']))]\n\nlen(english)","metadata":{"id":"nB6fMfEyqMUR","outputId":"8adbf8cc-6065-4044-b1c9-4384053477cd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"english","metadata":{"id":"yAYB_9ErTTju","outputId":"088b3064-979a-4f4d-f47c-68168661db48"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим разбиение по классам, среди текстов с иностранными словами.","metadata":{"id":"am_hpqAOYlXK"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nlabel = [0, 1]\n\nlbls = english['label'].unique()\n\ncntr = []\nfor label in lbls:\n    cntr.append(len(english[english['label'] == label]))\n\nplt.bar(lbls, cntr)\nplt.xticks(lbls)\nplt.plot()","metadata":{"id":"qXbd6jHIXbrZ","outputId":"a1ccbaac-027e-4e25-d696-0358e7f828a0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Попробуем перевести все тексты с английского на Русский без выделения именованных сущностей (названия компаний, имена, фамилии)","metadata":{"id":"hBfB9hRUc2B_"}},{"cell_type":"markdown","source":"**Выполним токенизацию**","metadata":{"id":"VbFXci_4g_Kc"}},{"cell_type":"markdown","source":"Токенизируем наши предложения по словам, удалим стоп-слова и приведем слова в начальную форму.","metadata":{"id":"Dm14E5tktNhV"}},{"cell_type":"code","source":"from razdel import tokenize\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nimport pymorphy2\n\nmorph = pymorphy2.MorphAnalyzer()\n\nstopwords.words('english')\n\nr_russian = re.compile('[А-Яа-я]')\nr_english = re.compile('[A-Za-z]')\n\n\ntokenize_texts = []\nfor item in tqdm(train_data['title']):\n\n    t_tokens = [_.text for _ in tokenize(item)]\n\n    tokens = []\n\n    for token in t_tokens:\n        if (token not in stopwords.words('russian')) and (\n            token not in stopwords.words('english')):\n\n            if (re.match(r_russian, token)) or (re.match(r_english, token)):\n                tokens.append(morph.parse(token)[0].normal_form)\n\n    tokenize_texts.append(tokens)","metadata":{"id":"-pQh9HpShPsQ","outputId":"01abac58-8967-4e5b-ec12-baa25acde859"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['title'] = tokenize_texts","metadata":{"id":"ndNCeCxojpQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"id":"PII8yXAl3r8y","outputId":"159ce2e6-099b-459b-816a-d225cad7b00d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"id":"Y3dSF-0rjTh0","outputId":"2cc07e89-b55f-486d-f008-c5d8cbe00a83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['title'].str.len() < 5]","metadata":{"id":"g0OTZYgcitsp","outputId":"7c46b925-d4ab-4972-814a-db733743471a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"id":"AgAyYGBs23bF","outputId":"d595ff28-7ed9-4973-813d-cd75fd7f91e6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"25000 записей имеют длину title менее 5 слов, пока что фильтровать не будем, так как для определения порнографического контента в теории может быть достаточно 1-2 слов.","metadata":{"id":"0LTGqFRUjtfe"}},{"cell_type":"markdown","source":"Сплитанем url по точкам, чтобы получить слова или что-то похожее на них. Переводить смысла нет, так как много ссылок выглядят как русские слова написанные транслитом.","metadata":{"id":"7qzo5emFk3Nw"}},{"cell_type":"code","source":"splitted_url = [url.split('.') for url in train_data['url']]\ntrain_data['url'] = splitted_url","metadata":{"id":"NSiWah1Ok2dB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = train_data['label']\nX = train_data.drop(['label', 'ID'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,\n                                                    random_state=42, stratify=y)","metadata":{"id":"tu2fP5R1ln4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['label'] == 1]","metadata":{"id":"lPVIZxEImNKh","outputId":"20461374-1c83-4cad-ae22-d4a4bc06a913"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['label'] == 0]","metadata":{"id":"RG-XSXTypB7b","outputId":"d105d9f6-926d-4af4-ed93-8d7190586ce2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наблюдается сильный дизбаланс классов. Примеров 1 класса в 7 раз меньше, чем 0 класса. Попробуем использовать CatBoost в данной ситуации, а при разделении X, y на трейн и тест будем использовать стратификацию, чтобы сохранить отношение количества классов в каждой из них","metadata":{"id":"zx5kG-KSpF-e"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(min_df=10, max_df=0.5)\n\nX_title = [' '.join(title) for title in X_train['title']]\n\nX_title_vectorized = vectorizer.fit_transform(X_title)\n\nX_title_vectorized","metadata":{"id":"afrGNvRBpnux","outputId":"df123dd1-3222-429b-901a-3eafcab88496"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install catboost","metadata":{"id":"90qi5xjUCFgI","outputId":"9858c8af-e47b-4a66-dac0-1c599ac7a38c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_title_eval = [' '.join(title) for title in X_test['title']]\n\nX_title_test = vectorizer.transform(X_title_eval)","metadata":{"id":"LVxyfuMUCqZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(use_best_model=False,\n                           depth=6,\n                           iterations=3000)\n\nmodel.fit(X_title_vectorized, y_train)","metadata":{"id":"qgm8bZdnA2Th"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost.utils import get_roc_curve\nimport sklearn\nfrom sklearn import metrics\nfrom catboost import Pool\n\neval_pool = Pool(X_title_test, y_test)\ncurve = get_roc_curve(model, eval_pool)\n(fpr, tpr, thresholds) = curve\nroc_auc = sklearn.metrics.auc(fpr, tpr)","metadata":{"id":"1asUYmUXIULu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc","metadata":{"id":"rJ-yK_TDIs2a","outputId":"f2f70f1d-710e-4100-e08c-ce32cda7e117"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\npred = model.predict(X_title_test)\n\nreport = classification_report(y_test, pred)\nprint(report)","metadata":{"id":"WYsdFvMcLpBD","outputId":"7062e513-7b89-42bd-e43c-816eb5f9b034"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/otbrniy/test.csv/test.csv'\n\ntest_data = pd.read_csv(path)","metadata":{"id":"GzUiBt4pN8sM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = re.compile('[^A-Za-zА-Яа-я ]')\n\ncleaned_text = [re.sub(r, '', text.lower()) for text in test_data['title']]\n\nr = re.compile(' +')\ncleaned_text = [re.sub(r, ' ', text) for text in cleaned_text]\n\ntest_data['title'] = cleaned_text\n\nmorph = pymorphy2.MorphAnalyzer()\n\nstopwords.words('english')\n\nr_russian = re.compile('[А-Яа-я]')\nr_english = re.compile('[A-Za-z]')\n\n\ntokenize_texts = []\nfor item in tqdm(test_data['title']):\n\n    t_tokens = [_.text for _ in tokenize(item)]\n\n    tokens = []\n\n    for token in t_tokens:\n        if (token not in stopwords.words('russian')) and (\n            token not in stopwords.words('english')):\n\n            if (re.match(r_russian, token)) or (re.match(r_english, token)):\n                tokens.append(morph.parse(token)[0].normal_form)\n\n    tokenize_texts.append(tokens)\n\ntest_data['title'] = tokenize_texts\n\nX_title_test = [' '.join(title) for title in test_data['title']]\n\nX_title_test = vectorizer.transform(X_title_test)\n\npred = model.predict(X_title_test)\n\ntest_data['label'] = pred\n\ntest_data[['ID', 'label']].to_csv('ml_kirichenko_catboost.csv', index=False)","metadata":{"id":"iMGmaRjBOEUB","outputId":"271e9c14-42c0-40d7-c977-8ee8db960dbc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_title_test = [' '.join(title) for title in test_data['title']]\n\nX_title_test = vectorizer.transform(X_title_test)\n\npred = model.predict(X_title_test)\n\ntest_data['label'] = pred\n\ntest_data[['ID', 'label']].to_csv('ml_kirichenko_catboost.csv', index=False)","metadata":{"id":"OPaIskK5TCyk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Попробуем подобрать параметры через GridSearch для CatBoost","metadata":{"id":"3nn_aEg5v1EJ"}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier()\n\n# model.fit(X_title_vectorized, y_train)\n\ngrid = {'learning_rate': [0.03, 0.1],\n        'depth': [4, 6],\n        'l2_leaf_reg': [1, 3, 5]}\n\ngrid_search_result = model.grid_search(grid,\n                                       X=X_title_vectorized,\n                                       y=y_train,\n                                       plot=True)","metadata":{"id":"KXvN-Q5OTY6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"mXb20CoFQla-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search_result","metadata":{"id":"ObiUgFKlwj0h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посчитаем самые популярные слова для 1-й категории в датасете","metadata":{"id":"lFfKwcSrxyFa"}},{"cell_type":"code","source":"X_train[X_train['label'] == 1]['title'].value_counts()","metadata":{"id":"Ss_EQfYVx3xv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Решение через BERT**","metadata":{"id":"8htmGHuAHBLo"}},{"cell_type":"code","source":"!pip install transformers\n!pip install datasets","metadata":{"id":"RWzqlnIuIM8Y","outputId":"caa5e139-6ae8-45d1-da51-35a1774f80ce","execution":{"iopub.status.busy":"2023-11-17T12:47:30.906852Z","iopub.execute_input":"2023-11-17T12:47:30.907145Z","iopub.status.idle":"2023-11-17T12:47:54.563180Z","shell.execute_reply.started":"2023-11-17T12:47:30.907119Z","shell.execute_reply":"2023-11-17T12:47:54.562133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://huggingface.co/DeepPavlov/rubert-base-cased-sentence","metadata":{"id":"KnGlTu3fJuQJ","outputId":"c80cde01-fc1c-4677-ccd2-e7a824f5cea7","execution":{"iopub.status.busy":"2023-11-17T12:47:54.565131Z","iopub.execute_input":"2023-11-17T12:47:54.565464Z","iopub.status.idle":"2023-11-17T12:48:38.314525Z","shell.execute_reply.started":"2023-11-17T12:47:54.565431Z","shell.execute_reply":"2023-11-17T12:48:38.313373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom transformers import AutoModel, BertTokenizer, BertForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric, Dataset\nfrom sklearn.metrics import classification_report, f1_score\nfrom torch.utils.data import Dataset\nimport torch\n\nmodel = BertForSequenceClassification.from_pretrained('/kaggle/working/rubert-base-cased-sentence', num_labels=2).to(\"cuda\")\ntokenizer = BertTokenizer.from_pretrained('/kaggle/working/rubert-base-cased-sentence')","metadata":{"id":"7utVtMnyHKl7","outputId":"5b3ffd33-ab01-4e0c-f7d0-a0baf0316f7f","execution":{"iopub.status.busy":"2023-11-17T17:46:09.302562Z","iopub.execute_input":"2023-11-17T17:46:09.302958Z","iopub.status.idle":"2023-11-17T17:46:11.771383Z","shell.execute_reply.started":"2023-11-17T17:46:09.302923Z","shell.execute_reply":"2023-11-17T17:46:11.770340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/otbrniy/train.csv/train.csv'\n\ntrain_data = pd.read_csv(path)","metadata":{"id":"_TZ7B8irI5QR","execution":{"iopub.status.busy":"2023-11-17T17:46:11.773500Z","iopub.execute_input":"2023-11-17T17:46:11.776183Z","iopub.status.idle":"2023-11-17T17:46:12.234549Z","shell.execute_reply.started":"2023-11-17T17:46:11.776138Z","shell.execute_reply":"2023-11-17T17:46:12.233374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splitted_url = [url.split('.') for url in train_data['url']]\ntrain_data['url'] = splitted_url","metadata":{"id":"nyDuIpcMLtoW","execution":{"iopub.status.busy":"2023-11-17T17:46:12.236137Z","iopub.execute_input":"2023-11-17T17:46:12.236563Z","iopub.status.idle":"2023-11-17T17:46:12.375233Z","shell.execute_reply.started":"2023-11-17T17:46:12.236518Z","shell.execute_reply":"2023-11-17T17:46:12.374010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_marked_text(train_data):\n    marked_text = []\n    for index, row in train_data.iterrows():\n        words_in_url = []\n        for word in row['url']:\n            if len(word) > 3:\n                words_in_url.append(word)\n        \n        marked_text.append('[sep]' + ' '.join(words_in_url) + '[sep]'\n                           + str(row['title']))\n\n    return marked_text","metadata":{"id":"ukm710xmL4dJ","execution":{"iopub.status.busy":"2023-11-17T17:46:22.278362Z","iopub.execute_input":"2023-11-17T17:46:22.279211Z","iopub.status.idle":"2023-11-17T17:46:22.286499Z","shell.execute_reply.started":"2023-11-17T17:46:22.279174Z","shell.execute_reply":"2023-11-17T17:46:22.285291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = train_data['label']\nX = train_data.drop(['label', 'ID'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,\n                                                    random_state=42, stratify=y)\n\nX_train['title'] = get_marked_text(X_train)\nX_test['title'] = get_marked_text(X_test)\nX_train.drop(['url'], axis=1)\nX_test.drop(['url'], axis=1)","metadata":{"id":"jZ6fhaBxSBZY","outputId":"67eb4522-4abf-49ec-9c1c-23a42c858978","execution":{"iopub.status.busy":"2023-11-17T17:46:23.455655Z","iopub.execute_input":"2023-11-17T17:46:23.456543Z","iopub.status.idle":"2023-11-17T17:46:31.869985Z","shell.execute_reply.started":"2023-11-17T17:46:23.456505Z","shell.execute_reply":"2023-11-17T17:46:31.868870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text = X_train['title'].to_list()","metadata":{"id":"QL4KxcE3V6Cf","execution":{"iopub.status.busy":"2023-11-17T17:46:34.264851Z","iopub.execute_input":"2023-11-17T17:46:34.265701Z","iopub.status.idle":"2023-11-17T17:46:34.276644Z","shell.execute_reply.started":"2023-11-17T17:46:34.265668Z","shell.execute_reply":"2023-11-17T17:46:34.275646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text = X_test['title'].to_list()","metadata":{"id":"vjSwnTNFX6U3","execution":{"iopub.status.busy":"2023-11-17T17:46:35.538326Z","iopub.execute_input":"2023-11-17T17:46:35.539167Z","iopub.status.idle":"2023-11-17T17:46:35.547887Z","shell.execute_reply.started":"2023-11-17T17:46:35.539129Z","shell.execute_reply":"2023-11-17T17:46:35.546751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.to_list()\ny_test = y_test.to_list()","metadata":{"id":"mtlSd9-XYzBm","execution":{"iopub.status.busy":"2023-11-17T17:46:49.330400Z","iopub.execute_input":"2023-11-17T17:46:49.330766Z","iopub.status.idle":"2023-11-17T17:46:49.340183Z","shell.execute_reply.started":"2023-11-17T17:46:49.330732Z","shell.execute_reply":"2023-11-17T17:46:49.339034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_len = 512","metadata":{"id":"gZlorTpbRueM","execution":{"iopub.status.busy":"2023-11-17T17:46:48.363987Z","iopub.execute_input":"2023-11-17T17:46:48.364487Z","iopub.status.idle":"2023-11-17T17:46:48.370982Z","shell.execute_reply.started":"2023-11-17T17:46:48.364450Z","shell.execute_reply":"2023-11-17T17:46:48.369968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens_train = tokenizer.batch_encode_plus(\n    train_text,\n    max_length = max_seq_len,\n    padding = 'max_length',\n    truncation = True\n)\ntokens_test = tokenizer.batch_encode_plus(\n    test_text,\n    max_length = max_seq_len,\n    padding = 'max_length',\n    truncation = True\n)","metadata":{"id":"WPOmrLbeRwFS","execution":{"iopub.status.busy":"2023-11-17T12:49:11.712742Z","iopub.execute_input":"2023-11-17T12:49:11.713178Z","iopub.status.idle":"2023-11-17T12:50:35.910882Z","shell.execute_reply.started":"2023-11-17T12:49:11.713151Z","shell.execute_reply":"2023-11-17T12:50:35.909859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Data(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n        item[\"labels\"] = torch.tensor([self.labels[idx]])\n        return item\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = Data(tokens_train, y_train)\ntest_dataset = Data(tokens_test, y_test)","metadata":{"id":"4hbrvz1URyOw","execution":{"iopub.status.busy":"2023-11-17T12:50:35.912178Z","iopub.execute_input":"2023-11-17T12:50:35.912489Z","iopub.status.idle":"2023-11-17T12:50:35.922889Z","shell.execute_reply.started":"2023-11-17T12:50:35.912462Z","shell.execute_reply":"2023-11-17T12:50:35.921843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, preds, average='weighted')\n    return {'F1': f1}\n# metric = load_metric('precision', average='weighted')\n# def compute_metrics(eval_pred):\n#     logits, labels = eval_pred\n#     predictions = np.argmax(logits, axis=-1)\n#     return metric.compute(predictions=predictions, references=labels)","metadata":{"id":"TYMFmOAKSq4T","execution":{"iopub.status.busy":"2023-11-17T12:50:35.923966Z","iopub.execute_input":"2023-11-17T12:50:35.924215Z","iopub.status.idle":"2023-11-17T12:50:35.930331Z","shell.execute_reply.started":"2023-11-17T12:50:35.924193Z","shell.execute_reply":"2023-11-17T12:50:35.929600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch]","metadata":{"id":"685x1x-uS1Jz","execution":{"iopub.status.busy":"2023-11-17T12:50:35.931278Z","iopub.execute_input":"2023-11-17T12:50:35.931565Z","iopub.status.idle":"2023-11-17T12:50:47.562640Z","shell.execute_reply.started":"2023-11-17T12:50:35.931541Z","shell.execute_reply":"2023-11-17T12:50:47.561640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import accelerate\naccelerate.__version__","metadata":{"id":"1JXNUXGfTY63","outputId":"06abebc3-cc40-4abb-e725-bda07dcde01f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = './results', #Выходной каталог\n    num_train_epochs = 2, #Кол-во эпох для обучения\n    per_device_train_batch_size = 8, #Размер пакета для каждого устройства во время обучения\n    per_device_eval_batch_size = 8, #Размер пакета для каждого устройства во время валидации\n    weight_decay = 0.01, #Понижение весов\n    logging_dir = './logs', #Каталог для хранения журналов\n    load_best_model_at_end = True, #Загружать ли лучшую модель после обучения\n    learning_rate = 1e-5, #Скорость обучения\n    evaluation_strategy ='epoch', #Валидация после каждой эпохи (можно сделать после конкретного кол-ва шагов)\n    logging_strategy = 'epoch', #Логирование после каждой эпохи\n    save_strategy = 'epoch', #Сохранение после каждой эпохи\n    save_total_limit = 1,\n    seed=21)","metadata":{"id":"HPVLuXMFSv8R","execution":{"iopub.status.busy":"2023-11-17T12:50:47.564088Z","iopub.execute_input":"2023-11-17T12:50:47.564427Z","iopub.status.idle":"2023-11-17T12:50:47.575387Z","shell.execute_reply.started":"2023-11-17T12:50:47.564392Z","shell.execute_reply":"2023-11-17T12:50:47.574648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model=model,\n                  tokenizer = tokenizer,\n                  args = training_args,\n                  train_dataset = train_dataset,\n                  eval_dataset = train_dataset,\n                  compute_metrics = compute_metrics)","metadata":{"id":"pMAVkz0EVHxZ","execution":{"iopub.status.busy":"2023-11-17T12:50:47.576418Z","iopub.execute_input":"2023-11-17T12:50:47.576719Z","iopub.status.idle":"2023-11-17T12:50:48.140477Z","shell.execute_reply.started":"2023-11-17T12:50:47.576682Z","shell.execute_reply":"2023-11-17T12:50:48.139713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.iloc[11318]","metadata":{"id":"ZyHdlzmpYj5e","outputId":"7e928872-5e0d-4e92-e7a6-ee4e29727ab1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"Duf-BGnZVK9b","outputId":"0586653b-7705-41d9-e55e-8e111ee0d3ce","execution":{"iopub.status.busy":"2023-11-17T12:50:48.141527Z","iopub.execute_input":"2023-11-17T12:50:48.141780Z","iopub.status.idle":"2023-11-17T16:39:14.974222Z","shell.execute_reply.started":"2023-11-17T12:50:48.141757Z","shell.execute_reply":"2023-11-17T16:39:14.972891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/results","metadata":{"id":"yiywp5ahQG6U","execution":{"iopub.status.busy":"2023-11-17T17:14:20.412817Z","iopub.execute_input":"2023-11-17T17:14:20.413705Z","iopub.status.idle":"2023-11-17T17:15:53.659514Z","shell.execute_reply.started":"2023-11-17T17:14:20.413669Z","shell.execute_reply":"2023-11-17T17:15:53.658393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r rubert-base-cased-sentence /kaggle/working/rubert-base-cased-sentence","metadata":{"id":"5kuQkaXi4Bs8","execution":{"iopub.status.busy":"2023-11-17T17:19:24.433046Z","iopub.execute_input":"2023-11-17T17:19:24.434128Z","iopub.status.idle":"2023-11-17T17:21:53.052130Z","shell.execute_reply.started":"2023-11-17T17:19:24.434083Z","shell.execute_reply":"2023-11-17T17:21:53.051015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"/kaggle/working/results/checkpoint-22666\")\ntokenizer = tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/results/checkpoint-22666\")","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:49:26.344172Z","iopub.execute_input":"2023-11-17T17:49:26.344559Z","iopub.status.idle":"2023-11-17T17:49:28.243454Z","shell.execute_reply.started":"2023-11-17T17:49:26.344523Z","shell.execute_reply":"2023-11-17T17:49:28.242354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nwith torch.no_grad():\n    variance = []\n    for text in tqdm(test_text):\n        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True).to(model.device)\n        proba = torch.sigmoid(model(**inputs).logits).cpu().numpy()\n        variance.append(proba.dot([0, 1]))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:50:16.301677Z","iopub.execute_input":"2023-11-17T17:50:16.302078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\n\nfor item in variance:\n    if item > 0.7:\n        pred.append(1)\n    else:\n        pred.append(0)\n\ntest_data['label'] = pred\n\ntest_data[['ID', 'label']].to_csv('ml_kirichenko_catboost.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-17T17:02:18.031895Z","iopub.status.idle":"2023-11-17T17:02:18.032347Z","shell.execute_reply.started":"2023-11-17T17:02:18.032083Z","shell.execute_reply":"2023-11-17T17:02:18.032104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RubertTiny**","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n\n  def __init__(self, texts, targets, tokenizer, max_len=512):\n    self.texts = texts\n    self.targets = targets\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n\n  def __len__(self):\n    return len(self.texts)\n\n  def __getitem__(self, idx):\n    text = str(self.texts[idx])\n    target = self.targets[idx]\n\n    encoding = self.tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        padding='max_length',\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n\n    return {\n      'text': text,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.long)\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://huggingface.co/cointegrated/rubert-tiny2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification\ntokenizer_path = 'cointegrated/rubert-tiny'\nmodel_path = 'cointegrated/rubert-tiny'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data DataLoader\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.metrics import f1_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertClassifier:\n\n    def __init__(self, model_path, tokenizer_path, n_classes=2, epochs=1, model_save_path='/content/bert.pt'):\n        self.model = BertForSequenceClassification.from_pretrained(model_path)\n        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.model_save_path=model_save_path\n        self.max_len = 512\n        self.epochs = epochs\n        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n        self.model.to(self.device)\n        \n    def preparation(self, X_train, y_train, X_valid, y_valid):\n        # create datasets\n        self.train_set = CustomDataset(X_train, y_train, self.tokenizer)\n        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer)\n\n        # create data loaders\n        self.train_loader = DataLoader(self.train_set, batch_size=2, shuffle=True)\n        self.valid_loader = DataLoader(self.valid_set, batch_size=2, shuffle=True)\n\n        # helpers initialization\n        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n        self.scheduler = get_linear_schedule_with_warmup(\n                self.optimizer,\n                num_warmup_steps=0,\n                num_training_steps=len(self.train_loader) * self.epochs\n            )\n        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n        \n    def fit(self):\n        self.model = self.model.train()\n        losses = []\n        correct_predictions = 0\n\n        for data in self.train_loader:\n            input_ids = data[\"input_ids\"].to(self.device)\n            attention_mask = data[\"attention_mask\"].to(self.device)\n            targets = data[\"targets\"].to(self.device)\n\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n                )\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            loss = self.loss_fn(outputs.logits, targets)\n\n            correct_predictions += torch.sum(preds == targets)\n\n            losses.append(loss.item())\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            self.scheduler.step()\n            self.optimizer.zero_grad()\n\n        train_f1 = f1_score(self.train_set, correct_predictions.double())\n        train_loss = np.mean(losses)\n        return train_f1, train_loss\n    \n    def eval(self):\n        self.model = self.model.eval()\n        losses = []\n        correct_predictions = 0\n\n        with torch.no_grad():\n            for data in self.valid_loader:\n                input_ids = data[\"input_ids\"].to(self.device)\n                attention_mask = data[\"attention_mask\"].to(self.device)\n                targets = data[\"targets\"].to(self.device)\n\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask\n                    )\n\n                preds = torch.argmax(outputs.logits, dim=1)\n                loss = self.loss_fn(outputs.logits, targets)\n                correct_predictions += torch.sum(preds == targets)\n                losses.append(loss.item())\n\n        val_f1 = f1_score(self.valid_set, correct_predictions.double())\n        val_loss = np.mean(losses)\n        return val_f1, val_loss\n    \n    def train(self):\n        best_accuracy = 0\n        for epoch in range(self.epochs):\n            print(f'Epoch {epoch + 1}/{self.epochs}')\n            train_f1, train_loss = self.fit()\n            print(f'Train loss {train_loss} f1 {train_acc}')\n\n            val_f1, val_loss = self.eval()\n            print(f'Val loss {val_loss} f1 {val_f1}')\n            print('-' * 10)\n\n            if val_acc > best_accuracy:\n                torch.save(self.model, self.model_save_path)\n                best_accuracy = val_f1\n\n        self.model = torch.load(self.model_save_path)\n    \n    def predict(self, text):\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        out = {\n              'text': text,\n              'input_ids': encoding['input_ids'].flatten(),\n              'attention_mask': encoding['attention_mask'].flatten()\n          }\n\n        input_ids = out[\"input_ids\"].to(self.device)\n        attention_mask = out[\"attention_mask\"].to(self.device)\n\n        outputs = self.model(\n            input_ids=input_ids.unsqueeze(0),\n            attention_mask=attention_mask.unsqueeze(0)\n        )\n\n        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n\n        return prediction","metadata":{},"execution_count":null,"outputs":[]}]}